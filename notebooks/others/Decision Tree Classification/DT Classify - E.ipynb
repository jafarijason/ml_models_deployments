{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayesian (kNN)\n",
    "\n",
    "## Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateBinaryClassification(predictions, actuals):\n",
    "    contigency = pd.crosstab(actuals,predictions)\n",
    "    TP = contigency['yes']['yes']\n",
    "    TN = contigency['no']['no']\n",
    "    FP = contigency['yes']['no']\n",
    "    FN = contigency['no']['yes']\n",
    "    n = contigency.sum().sum()\n",
    "\n",
    "    Acuracy = (TP + TN)/n\n",
    "    Recall = TP/(TP+FN)\n",
    "    Precision = TP/(TP+FP)\n",
    "    FScore = 2*Recall*Precision/(Recall+Precision)\n",
    "    \n",
    "    return Acuracy, Recall, Precision, FScore\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df = pd.read_csv('Customer Subscription.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with unknowns\n",
    "customer_df.job.replace('unknown',np.nan,inplace=True)\n",
    "customer_df.marital.replace('unknown',np.nan,inplace=True)\n",
    "customer_df.education.replace('unknown',np.nan,inplace=True)\n",
    "customer_df.loan.replace('unknown',np.nan,inplace=True)\n",
    "customer_df.default.replace('unknown',np.nan,inplace=True)\n",
    "customer_df.job.replace('unknown',np.nan,inplace=True)\n",
    "customer_df.housing.replace('unknown',np.nan,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df.pdays.replace(999,np.nan,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First let's do KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task: Classificaiton\n",
    "\n",
    "We would like to predict the class (subscriber/ no subscriber) of customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_predictors = ['age', 'job', 'marital', 'education', 'default', 'housing', 'loan',\n",
    "       'contact', 'month', 'day_of_week', 'duration', 'campaign', 'pdays',\n",
    "       'previous', 'poutcome']\n",
    "target = 'y'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kNN Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=customer_df[target]\n",
    "\n",
    "Xs = pd.get_dummies(customer_df[possible_predictors],drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN can handle missing values, so we keep them as missing not to create bias in the data.\n",
    "\n",
    "However, the case of missing vlause for pdays is different. The values are not missing for our lack of knowlege, but they are missing for a difference about the population of data object that leads to them not having a value. In these situations, we will use MM method to fill the missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MM method\n",
    "\n",
    "we will fill the missing values with Max+Mean (MM) of the attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs.pdays.fillna(Xs.pdays.max()+Xs.pdays.mean(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize data\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "scaler.fit(Xs)  # Note the use of an array of column names\n",
    "\n",
    "Xs = pd.DataFrame(scaler.transform(Xs),columns =Xs.columns)\n",
    "Xs.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up experimentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(Xs,y,  test_size=0.3,random_state=1)\n",
    "\n",
    "print(X_train.shape,X_test.shape,y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=1000,random_state=2)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in rf.estimators_], axis=0)\n",
    "\n",
    "df = pd.DataFrame({'feature': X_train.columns, 'importance': importances, 'std': std})\n",
    "df = df.sort_values('importance')\n",
    "print(df)\n",
    "\n",
    "ax = df.plot(kind='barh', xerr='std', x='feature', legend=False)\n",
    "ax.set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_features=df[df.importance>=0.05].feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuned KNN\n",
    "Use the tune KNN to Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1,weights='uniform').fit(X_train[select_features], y_train)\n",
    "y_predict_knn = knn.predict(X_test[select_features])\n",
    "pd.crosstab(y_test,y_predict_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateBinaryClassification(y_predict_knn,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayesian\n",
    "\n",
    "## Preprocess\n",
    "\n",
    "NB can also handle missing values, but it does not need the data to be standardized. So some of the preprocessing steps will look different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=customer_df[target]\n",
    "\n",
    "Xs = pd.get_dummies(customer_df[possible_predictors],drop_first=True)\n",
    "Xs.pdays.fillna(Xs.pdays.max()+Xs.pdays.mean(),inplace=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xs,y,  test_size=0.3,random_state=1)\n",
    "print(X_train.shape,X_test.shape,y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same features we selected using Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train[select_features], y_train)\n",
    "\n",
    "y_predict_nb = nb.predict(X_test[select_features])\n",
    "pd.crosstab(y_test,y_predict_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateBinaryClassification(y_predict_nb,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities\n",
    "y_prob = nb.predict_proba(X_test[select_features])\n",
    "\n",
    "summary_df = pd.concat([pd.DataFrame({'actual': y_test, 'predicted': y_predict_nb}),\n",
    "                pd.DataFrame(y_prob, index=y_test.index,columns = ['No_prob','Yes_prob'])], axis=1)\n",
    "y_predict_nb = pd.Series(summary_df.Yes_prob>0.9999998888888889).replace({False:'no',True:'yes'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Methods = ['Random','KNN','NB','DT']\n",
    "Metrics = ['Accuracy','Recall','Precision','Fscore']\n",
    "\n",
    "compare_df = pd.DataFrame(index = Methods, columns = Metrics)\n",
    "\n",
    "#Method1 #KNN\n",
    "\n",
    "compare_df.loc['KNN'] = evaluateBinaryClassification(y_predict_knn,y_test)\n",
    "\n",
    "number_Yes =  np.sum(y_predict_knn=='yes')\n",
    "\n",
    "#Method 2 Random\n",
    "y_predict_random = pd.Series(np.random.permutation(len(y_test))<number_Yes).replace({False:'no',True:'yes'})\n",
    "print(evaluateBinaryClassification(y_predict_random,y_test))\n",
    "\n",
    "compare_df.loc['Random'] = evaluateBinaryClassification(y_predict_random,y_test)\n",
    "\n",
    "#Method 3 NB\n",
    "compare_df.loc['NB'] = evaluateBinaryClassification(y_predict_nb,y_test)\n",
    "compare_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Random Method number of yes prediction: {}'.format(np.sum(y_predict_random=='yes')))\n",
    "print('KNN Method number of yes prediction: {}'.format(np.sum(y_predict_knn=='yes')))\n",
    "print('NB Method number of yes prediction: {}'.format(np.sum(y_predict_nb=='yes')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DT\n",
    "Now let us start learning about DT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DT Preprocess\n",
    "Preprocessing is different for decision Tree as ordinal attributes needs to be transformed with ranking instead of binary coding. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=customer_df[target]\n",
    "\n",
    "Xs = pd.DataFrame(customer_df)\n",
    "Xs.pdays.fillna(Xs.pdays.max()+Xs.pdays.mean(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs.education.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_dic = {'basic.9y':3, 'university.degree':6, 'basic.4y':1, 'high.school':4,\n",
    "       'professional.course':5, 'basic.6y':2, 'illiterate':0}\n",
    "\n",
    "Xs.education.replace(replace_dic,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs.education.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = pd.get_dummies(Xs[possible_predictors],drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The module we are going to use cannot handle missing values, so we have to deal with them first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs.fillna(Xs.median(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(Xs,y,  test_size=0.3,random_state=1)\n",
    "print(X_train.shape,X_test.shape,y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from dmba import plotDecisionTree\n",
    "\n",
    "classTree = DecisionTreeClassifier()\n",
    "classTree.fit(X_train, y_train)\n",
    "\n",
    "print(\"Classes: {}\".format(', '.join(classTree.classes_)))\n",
    "plotDecisionTree(classTree, feature_names=X_train.columns, class_names=classTree.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_dt = classTree.predict(X_test)\n",
    "evaluateBinaryClassification(y_predict_dt,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Random Method number of yes prediction: {}'.format(np.sum(y_predict_random=='yes')))\n",
    "print('KNN Method number of yes prediction: {}'.format(np.sum(y_predict_knn=='yes')))\n",
    "print('NB Method number of yes prediction: {}'.format(np.sum(y_predict_nb=='yes')))\n",
    "print('DT Method number of yes prediction: {}'.format(np.sum(y_predict_dt=='yes')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_df.loc['DT'] = evaluateBinaryClassification(y_predict_dt,y_test)\n",
    "compare_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune DT\n",
    "parameters:\n",
    "\n",
    "### criterion{“gini”, “entropy”}, default=”gini”\n",
    "    The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain.\n",
    "\n",
    "### splitter{“best”, “random”}, default=”best”\n",
    "    The strategy used to choose the split at each node. Supported strategies are “best” to choose the best split and “random” to choose the best random split.\n",
    "\n",
    "### max_depth int, default=None\n",
    "    The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "\n",
    "### min_samples_split int or float, default=2\n",
    "    The minimum number of samples required to split an internal node:\n",
    "\n",
    "    If int, then consider min_samples_split as the minimum number.\n",
    "\n",
    "    If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.\n",
    "\n",
    "### min_impurity_decrease float, default=0.0\n",
    "    A node will be split if this split induces a decrease of the impurity greater than or equal to this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'criterion':['gini','entropy'],\n",
    "    'splitter' : ['best','random'],\n",
    "    'max_depth': [10, 20, 30, 40], \n",
    "    'min_samples_split': [20, 40, 60, 80, 100], \n",
    "    'min_impurity_decrease': [0, 0.0005, 0.001, 0.005, 0.01], \n",
    "}\n",
    "\n",
    "gridSearch = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5, scoring='recall')\n",
    "gridSearch.fit(X_train, y_train.replace({'yes':1,'no':0}))\n",
    "print('score: ', gridSearch.best_score_)\n",
    "print('parameters: ', gridSearch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'criterion':['entropy'],\n",
    "    'splitter' : ['best'],\n",
    "    'max_depth': [6,7,8,9,10,11,12,13,14], \n",
    "    'min_samples_split': [16,17,18,19,20,21,22,23,24,25], \n",
    "    'min_impurity_decrease': [0.001,0.003, 0.005,0.007,0.009], \n",
    "}\n",
    "\n",
    "gridSearch = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5, scoring='recall')\n",
    "gridSearch.fit(X_train, y_train.replace({'yes':1,'no':0}))\n",
    "print('score: ', gridSearch.best_score_)\n",
    "print('parameters: ', gridSearch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classTree = DecisionTreeClassifier(criterion= 'entropy', max_depth= 6,\n",
    "                                   min_impurity_decrease= 0.005, min_samples_split= 16, splitter= 'best')\n",
    "classTree.fit(X_train, y_train)\n",
    "\n",
    "plotDecisionTree(classTree, feature_names=X_train.columns, class_names=classTree.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_dt = classTree.predict(X_test)\n",
    "evaluateBinaryClassification(y_predict_dt,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comapre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_df.loc['DT'] = evaluateBinaryClassification(y_predict_dt,y_test)\n",
    "compare_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Random Method number of yes prediction: {}'.format(np.sum(y_predict_random=='yes')))\n",
    "print('KNN Method number of yes prediction: {}'.format(np.sum(y_predict_knn=='yes')))\n",
    "print('NB Method number of yes prediction: {}'.format(np.sum(y_predict_nb=='yes')))\n",
    "print('DT Method number of yes prediction: {}'.format(np.sum(y_predict_dt=='yes')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
